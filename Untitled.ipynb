{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f881ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enable_modelarts': 'Whether training on modelarts, default: False', 'data_url': 'Dataset url for obs', 'checkpoint_url': 'The location of checkpoint for obs', 'data_path': 'Dataset path for local', 'output_path': 'Training output path for local', 'load_path': 'The location of checkpoint for obs', 'device_target': 'Target device type, available: [Ascend, GPU, CPU]', 'enable_profiling': 'Whether enable profiling while training, default: False', 'num_classes': 'Class for dataset', 'batch_size': 'Batch size for training and evaluation', 'epoch_size': 'Total training epochs.', 'checkpoint_path': 'The location of the checkpoint file.', 'checkpoint_file_path': 'The location of the checkpoint file.', 'save_graphs': 'Whether save graphs during training, default: False.', 'save_graphs_path': 'Path to save graphs.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--config_path CONFIG_PATH]\n",
      "                   [--enable_modelarts ENABLE_MODELARTS] [--data_url DATA_URL]\n",
      "                   [--train_url TRAIN_URL] [--checkpoint_url CHECKPOINT_URL]\n",
      "                   [--run_distribute RUN_DISTRIBUTE]\n",
      "                   [--enable_profiling ENABLE_PROFILING]\n",
      "                   [--data_path DATA_PATH] [--output_path OUTPUT_PATH]\n",
      "                   [--load_path LOAD_PATH] [--device_target DEVICE_TARGET]\n",
      "                   [--checkpoint_path CHECKPOINT_PATH]\n",
      "                   [--checkpoint_file_path CHECKPOINT_FILE_PATH]\n",
      "                   [--optimizer OPTIMIZER] [--infer_label INFER_LABEL]\n",
      "                   [--class_num CLASS_NUM] [--batch_size BATCH_SIZE]\n",
      "                   [--loss_scale LOSS_SCALE] [--momentum MOMENTUM]\n",
      "                   [--weight_decay WEIGHT_DECAY] [--epoch_size EPOCH_SIZE]\n",
      "                   [--pretrain_epoch_size PRETRAIN_EPOCH_SIZE]\n",
      "                   [--save_checkpoint SAVE_CHECKPOINT]\n",
      "                   [--save_checkpoint_epochs SAVE_CHECKPOINT_EPOCHS]\n",
      "                   [--keep_checkpoint_max KEEP_CHECKPOINT_MAX]\n",
      "                   [--warmup_epochs WARMUP_EPOCHS]\n",
      "                   [--lr_decay_mode LR_DECAY_MODE]\n",
      "                   [--use_label_smooth USE_LABEL_SMOOTH]\n",
      "                   [--label_smooth_factor LABEL_SMOOTH_FACTOR]\n",
      "                   [--lr_init LR_INIT] [--lr_max LR_MAX] [--lr_end LR_END]\n",
      "                   [--lars_epsilon LARS_EPSILON]\n",
      "                   [--lars_coefficient LARS_COEFFICIENT] [--net_name NET_NAME]\n",
      "                   [--dataset DATASET] [--device_num DEVICE_NUM]\n",
      "                   [--pre_trained PRE_TRAINED] [--run_eval RUN_EVAL]\n",
      "                   [--eval_dataset_path EVAL_DATASET_PATH]\n",
      "                   [--parameter_server PARAMETER_SERVER]\n",
      "                   [--filter_weight FILTER_WEIGHT]\n",
      "                   [--save_best_ckpt SAVE_BEST_CKPT]\n",
      "                   [--eval_start_epoch EVAL_START_EPOCH]\n",
      "                   [--eval_interval EVAL_INTERVAL]\n",
      "                   [--enable_cache ENABLE_CACHE]\n",
      "                   [--cache_session_id CACHE_SESSION_ID]\n",
      "                   [--mode_name MODE_NAME] [--boost_mode BOOST_MODE]\n",
      "                   [--conv_init CONV_INIT] [--dense_init DENSE_INIT]\n",
      "                   [--train_image_size TRAIN_IMAGE_SIZE]\n",
      "                   [--eval_image_size EVAL_IMAGE_SIZE] [--device_id DEVICE_ID]\n",
      "                   [--width WIDTH] [--height HEIGHT] [--file_name FILE_NAME]\n",
      "                   [--file_format FILE_FORMAT] [--ckpt_file CKPT_FILE]\n",
      "                   [--network_dataset NETWORK_DATASET]\n",
      "                   [--save_graphs SAVE_GRAPHS]\n",
      "                   [--save_graphs_path SAVE_GRAPHS_PATH]\n",
      "                   [--has_trained_epoch HAS_TRAINED_EPOCH]\n",
      "                   [--has_trained_step HAS_TRAINED_STEP]\n",
      "                   [--result_path RESULT_PATH] [--label_path LABEL_PATH]\n",
      "__main__.py: error: unrecognized arguments: -f /home/ma-user/.local/share/jupyter/runtime/kernel-f06d4471-b193-4145-8b25-b02e9df025de.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020-2021 Huawei Technologies Co., Ltd\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ============================================================================\n",
    "\"\"\"ResNet.\"\"\"\n",
    "import math\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import mindspore.nn as nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import save_checkpoint\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.common.tensor import Tensor\n",
    "from config.resnet_config import config\n",
    "\n",
    "\n",
    "def conv_variance_scaling_initializer(in_channel, out_channel, kernel_size):\n",
    "    fan_in = in_channel * kernel_size * kernel_size\n",
    "    scale = 1.0\n",
    "    scale /= max(1., fan_in)\n",
    "    stddev = (scale ** 0.5) / .87962566103423978\n",
    "    if config.net_name == \"resnet152\":\n",
    "        stddev = (scale ** 0.5)\n",
    "    mu, sigma = 0, stddev\n",
    "    weight = truncnorm(-2, 2, loc=mu, scale=sigma).rvs(out_channel * in_channel * kernel_size * kernel_size)\n",
    "    weight = np.reshape(weight, (out_channel, in_channel, kernel_size, kernel_size))\n",
    "    return Tensor(weight, dtype=mstype.float32)\n",
    "\n",
    "\n",
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def calculate_gain(nonlinearity, param=None):\n",
    "    \"\"\"calculate_gain\"\"\"\n",
    "    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']\n",
    "    res = 0\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        res = 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        res = 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        res = math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            neg_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            neg_slope = param\n",
    "        else:\n",
    "            raise ValueError(\"neg_slope {} not a valid number\".format(param))\n",
    "        res = math.sqrt(2.0 / (1 + neg_slope ** 2))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n",
    "    return res\n",
    "\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    \"\"\"_calculate_fan_in_and_fan_out\"\"\"\n",
    "    dimensions = len(tensor)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "    if dimensions == 2:  # Linear\n",
    "        fan_in = tensor[1]\n",
    "        fan_out = tensor[0]\n",
    "    else:\n",
    "        num_input_fmaps = tensor[1]\n",
    "        num_output_fmaps = tensor[0]\n",
    "        receptive_field_size = 1\n",
    "        if dimensions > 2:\n",
    "            receptive_field_size = tensor[2] * tensor[3]\n",
    "        fan_in = num_input_fmaps * receptive_field_size\n",
    "        fan_out = num_output_fmaps * receptive_field_size\n",
    "    return fan_in, fan_out\n",
    "\n",
    "\n",
    "def _calculate_correct_fan(tensor, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(\"Unsupported mode {}, please use one of {}\".format(mode, valid_modes))\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "\n",
    "def kaiming_normal(inputs_shape, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "    fan = _calculate_correct_fan(inputs_shape, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, size=inputs_shape).astype(np.float32)\n",
    "\n",
    "\n",
    "def kaiming_uniform(inputs_shape, a=0., mode='fan_in', nonlinearity='leaky_relu'):\n",
    "    fan = _calculate_correct_fan(inputs_shape, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n",
    "    return np.random.uniform(-bound, bound, size=inputs_shape).astype(np.float32)\n",
    "\n",
    "\n",
    "def _conv3x3(in_channel, out_channel, stride=1, use_se=False, res_base=False):\n",
    "    if use_se:\n",
    "        weight = conv_variance_scaling_initializer(in_channel, out_channel, kernel_size=3)\n",
    "    else:\n",
    "        weight_shape = (out_channel, in_channel, 3, 3)\n",
    "        weight = Tensor(kaiming_normal(weight_shape, mode=\"fan_out\", nonlinearity='relu'))\n",
    "        if config.net_name == \"resnet152\":\n",
    "            weight = _weight_variable(weight_shape)\n",
    "    if res_base:\n",
    "        return nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride,\n",
    "                         padding=1, pad_mode='pad', weight_init=weight)\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride,\n",
    "                     padding=0, pad_mode='same', weight_init=weight)\n",
    "\n",
    "\n",
    "def _conv1x1(in_channel, out_channel, stride=1, use_se=False, res_base=False):\n",
    "    if use_se:\n",
    "        weight = conv_variance_scaling_initializer(in_channel, out_channel, kernel_size=1)\n",
    "    else:\n",
    "        weight_shape = (out_channel, in_channel, 1, 1)\n",
    "        weight = Tensor(kaiming_normal(weight_shape, mode=\"fan_out\", nonlinearity='relu'))\n",
    "        if config.net_name == \"resnet152\":\n",
    "            weight = _weight_variable(weight_shape)\n",
    "    if res_base:\n",
    "        return nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride,\n",
    "                         padding=0, pad_mode='pad', weight_init=weight)\n",
    "    return nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride,\n",
    "                     padding=0, pad_mode='same', weight_init=weight)\n",
    "\n",
    "\n",
    "def _conv7x7(in_channel, out_channel, stride=1, use_se=False, res_base=False):\n",
    "    if use_se:\n",
    "        weight = conv_variance_scaling_initializer(in_channel, out_channel, kernel_size=7)\n",
    "    else:\n",
    "        weight_shape = (out_channel, in_channel, 7, 7)\n",
    "        weight = Tensor(kaiming_normal(weight_shape, mode=\"fan_out\", nonlinearity='relu'))\n",
    "        if config.net_name == \"resnet152\":\n",
    "            weight = _weight_variable(weight_shape)\n",
    "    if res_base:\n",
    "        return nn.Conv2d(in_channel, out_channel,\n",
    "                         kernel_size=7, stride=stride, padding=3, pad_mode='pad', weight_init=weight)\n",
    "    return nn.Conv2d(in_channel, out_channel,\n",
    "                     kernel_size=7, stride=stride, padding=0, pad_mode='same', weight_init=weight)\n",
    "\n",
    "\n",
    "def _bn(channel, res_base=False):\n",
    "    if res_base:\n",
    "        return nn.BatchNorm2d(channel, eps=1e-5, momentum=0.1,\n",
    "                              gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "    return nn.BatchNorm2d(channel, eps=1e-4, momentum=0.9,\n",
    "                          gamma_init=1, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _bn_last(channel):\n",
    "    return nn.BatchNorm2d(channel, eps=1e-4, momentum=0.9,\n",
    "                          gamma_init=0, beta_init=0, moving_mean_init=0, moving_var_init=1)\n",
    "\n",
    "\n",
    "def _fc(in_channel, out_channel, use_se=False):\n",
    "    if use_se:\n",
    "        weight = np.random.normal(loc=0, scale=0.01, size=out_channel * in_channel)\n",
    "        weight = Tensor(np.reshape(weight, (out_channel, in_channel)), dtype=mstype.float32)\n",
    "    else:\n",
    "        weight_shape = (out_channel, in_channel)\n",
    "        weight = Tensor(kaiming_uniform(weight_shape, a=math.sqrt(5)))\n",
    "        if config.net_name == \"resnet152\":\n",
    "            weight = _weight_variable(weight_shape)\n",
    "    return nn.Dense(in_channel, out_channel, has_bias=True, weight_init=weight, bias_init=0)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResNet V1 residual block definition.\n",
    "\n",
    "    Args:\n",
    "        in_channel (int): Input channel.\n",
    "        out_channel (int): Output channel.\n",
    "        stride (int): Stride size for the first convolutional layer. Default: 1.\n",
    "        use_se (bool): Enable SE-ResNet50 net. Default: False.\n",
    "        se_block(bool): Use se block in SE-ResNet50 net. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ResidualBlock(3, 256, stride=2)\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channel,\n",
    "                 out_channel,\n",
    "                 stride=1,\n",
    "                 use_se=False, se_block=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.use_se = use_se\n",
    "        self.se_block = se_block\n",
    "        channel = out_channel // self.expansion\n",
    "        self.conv1 = _conv1x1(in_channel, channel, stride=1, use_se=self.use_se)\n",
    "        self.bn1 = _bn(channel)\n",
    "        if self.use_se and self.stride != 1:\n",
    "            self.e2 = nn.SequentialCell([_conv3x3(channel, channel, stride=1, use_se=True), _bn(channel),\n",
    "                                         nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='same')])\n",
    "        else:\n",
    "            self.conv2 = _conv3x3(channel, channel, stride=stride, use_se=self.use_se)\n",
    "            self.bn2 = _bn(channel)\n",
    "\n",
    "        self.conv3 = _conv1x1(channel, out_channel, stride=1, use_se=self.use_se)\n",
    "        self.bn3 = _bn(out_channel)\n",
    "        if config.optimizer == \"Thor\" or config.net_name == \"resnet152\" or config.net_name == \"resnet101\":\n",
    "            self.bn3 = _bn_last(out_channel)\n",
    "        if self.se_block:\n",
    "            self.se_global_pool = P.ReduceMean(keep_dims=False)\n",
    "            self.se_dense_0 = _fc(out_channel, int(out_channel / 4), use_se=self.use_se)\n",
    "            self.se_dense_1 = _fc(int(out_channel / 4), out_channel, use_se=self.use_se)\n",
    "            self.se_sigmoid = nn.Sigmoid()\n",
    "            self.se_mul = P.Mul()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.down_sample = False\n",
    "\n",
    "        if stride != 1 or in_channel != out_channel:\n",
    "            self.down_sample = True\n",
    "        self.down_sample_layer = None\n",
    "\n",
    "        if self.down_sample:\n",
    "            if self.use_se:\n",
    "                if stride == 1:\n",
    "                    self.down_sample_layer = nn.SequentialCell([_conv1x1(in_channel, out_channel,\n",
    "                                                                         stride, use_se=self.use_se), _bn(out_channel)])\n",
    "                else:\n",
    "                    self.down_sample_layer = nn.SequentialCell([nn.MaxPool2d(kernel_size=2, stride=2, pad_mode='same'),\n",
    "                                                                _conv1x1(in_channel, out_channel, 1,\n",
    "                                                                         use_se=self.use_se), _bn(out_channel)])\n",
    "            else:\n",
    "                self.down_sample_layer = nn.SequentialCell([_conv1x1(in_channel, out_channel, stride,\n",
    "                                                                     use_se=self.use_se), _bn(out_channel)])\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        if self.use_se and self.stride != 1:\n",
    "            out = self.e2(out)\n",
    "        else:\n",
    "            out = self.conv2(out)\n",
    "            out = self.bn2(out)\n",
    "            out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.se_block:\n",
    "            out_se = out\n",
    "            out = self.se_global_pool(out, (2, 3))\n",
    "            out = self.se_dense_0(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.se_dense_1(out)\n",
    "            out = self.se_sigmoid(out)\n",
    "            out = F.reshape(out, F.shape(out) + (1, 1))\n",
    "            out = self.se_mul(out, out_se)\n",
    "\n",
    "        if self.down_sample:\n",
    "            identity = self.down_sample_layer(identity)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlockBase(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResNet V1 residual block definition.\n",
    "\n",
    "    Args:\n",
    "        in_channel (int): Input channel.\n",
    "        out_channel (int): Output channel.\n",
    "        stride (int): Stride size for the first convolutional layer. Default: 1.\n",
    "        use_se (bool): Enable SE-ResNet50 net. Default: False.\n",
    "        se_block(bool): Use se block in SE-ResNet50 net. Default: False.\n",
    "        res_base (bool): Enable parameter setting of resnet18. Default: True.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ResidualBlockBase(3, 256, stride=2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channel,\n",
    "                 out_channel,\n",
    "                 stride=1,\n",
    "                 use_se=False,\n",
    "                 se_block=False,\n",
    "                 res_base=True):\n",
    "        super(ResidualBlockBase, self).__init__()\n",
    "        self.res_base = res_base\n",
    "        self.conv1 = _conv3x3(in_channel, out_channel, stride=stride, res_base=self.res_base)\n",
    "        self.bn1d = _bn(out_channel)\n",
    "        self.conv2 = _conv3x3(out_channel, out_channel, stride=1, res_base=self.res_base)\n",
    "        self.bn2d = _bn(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.down_sample = False\n",
    "        if stride != 1 or in_channel != out_channel:\n",
    "            self.down_sample = True\n",
    "\n",
    "        self.down_sample_layer = None\n",
    "        if self.down_sample:\n",
    "            self.down_sample_layer = nn.SequentialCell([_conv1x1(in_channel, out_channel, stride,\n",
    "                                                                 use_se=use_se, res_base=self.res_base),\n",
    "                                                        _bn(out_channel, res_base)])\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1d(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2d(out)\n",
    "\n",
    "        if self.down_sample:\n",
    "            identity = self.down_sample_layer(identity)\n",
    "\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    ResNet architecture.\n",
    "\n",
    "    Args:\n",
    "        block (Cell): Block for network.\n",
    "        layer_nums (list): Numbers of block in different layers.\n",
    "        in_channels (list): Input channel in each layer.\n",
    "        out_channels (list): Output channel in each layer.\n",
    "        strides (list):  Stride size in each layer.\n",
    "        num_classes (int): The number of classes that the training images are belonging to.\n",
    "        use_se (bool): Enable SE-ResNet50 net. Default: False.\n",
    "        se_block(bool): Use se block in SE-ResNet50 net in layer 3 and layer 4. Default: False.\n",
    "        res_base (bool): Enable parameter setting of resnet18. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor.\n",
    "\n",
    "    Examples:\n",
    "        >>> ResNet(ResidualBlock,\n",
    "        >>>        [3, 4, 6, 3],\n",
    "        >>>        [64, 256, 512, 1024],\n",
    "        >>>        [256, 512, 1024, 2048],\n",
    "        >>>        [1, 2, 2, 2],\n",
    "        >>>        10)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layer_nums,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 strides,\n",
    "                 num_classes,\n",
    "                 use_se=False,\n",
    "                 res_base=False):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        if not len(layer_nums) == len(in_channels) == len(out_channels) == 4:\n",
    "            raise ValueError(\"the length of layer_num, in_channels, out_channels list must be 4!\")\n",
    "        self.use_se = use_se\n",
    "        self.res_base = res_base\n",
    "        self.se_block = False\n",
    "        if self.use_se:\n",
    "            self.se_block = True\n",
    "\n",
    "        if self.use_se:\n",
    "            self.conv1_0 = _conv3x3(3, 32, stride=2, use_se=self.use_se)\n",
    "            self.bn1_0 = _bn(32)\n",
    "            self.conv1_1 = _conv3x3(32, 32, stride=1, use_se=self.use_se)\n",
    "            self.bn1_1 = _bn(32)\n",
    "            self.conv1_2 = _conv3x3(32, 64, stride=1, use_se=self.use_se)\n",
    "        else:\n",
    "            self.conv1 = _conv7x7(3, 64, stride=2, res_base=self.res_base)\n",
    "        self.bn1 = _bn(64, self.res_base)\n",
    "        self.relu = P.ReLU()\n",
    "\n",
    "        if self.res_base:\n",
    "            self.pad = nn.Pad(paddings=((0, 0), (0, 0), (1, 1), (1, 1)))\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"valid\")\n",
    "        else:\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode=\"same\")\n",
    "\n",
    "        self.layer1 = self._make_layer(block,\n",
    "                                       layer_nums[0],\n",
    "                                       in_channel=in_channels[0],\n",
    "                                       out_channel=out_channels[0],\n",
    "                                       stride=strides[0],\n",
    "                                       use_se=self.use_se)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       layer_nums[1],\n",
    "                                       in_channel=in_channels[1],\n",
    "                                       out_channel=out_channels[1],\n",
    "                                       stride=strides[1],\n",
    "                                       use_se=self.use_se)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       layer_nums[2],\n",
    "                                       in_channel=in_channels[2],\n",
    "                                       out_channel=out_channels[2],\n",
    "                                       stride=strides[2],\n",
    "                                       use_se=self.use_se,\n",
    "                                       se_block=self.se_block)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       layer_nums[3],\n",
    "                                       in_channel=in_channels[3],\n",
    "                                       out_channel=out_channels[3],\n",
    "                                       stride=strides[3],\n",
    "                                       use_se=self.use_se,\n",
    "                                       se_block=self.se_block)\n",
    "\n",
    "        self.mean = P.ReduceMean(keep_dims=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.end_point = _fc(out_channels[3], num_classes, use_se=self.use_se)\n",
    "\n",
    "    def _make_layer(self, block, layer_num, in_channel, out_channel, stride, use_se=False, se_block=False):\n",
    "        \"\"\"\n",
    "        Make stage network of ResNet.\n",
    "\n",
    "        Args:\n",
    "            block (Cell): Resnet block.\n",
    "            layer_num (int): Layer number.\n",
    "            in_channel (int): Input channel.\n",
    "            out_channel (int): Output channel.\n",
    "            stride (int): Stride size for the first convolutional layer.\n",
    "            se_block(bool): Use se block in SE-ResNet50 net. Default: False.\n",
    "        Returns:\n",
    "            SequentialCell, the output layer.\n",
    "\n",
    "        Examples:\n",
    "            >>> _make_layer(ResidualBlock, 3, 128, 256, 2)\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "\n",
    "        resnet_block = block(in_channel, out_channel, stride=stride, use_se=use_se)\n",
    "        layers.append(resnet_block)\n",
    "        if se_block:\n",
    "            for _ in range(1, layer_num - 1):\n",
    "                resnet_block = block(out_channel, out_channel, stride=1, use_se=use_se)\n",
    "                layers.append(resnet_block)\n",
    "            resnet_block = block(out_channel, out_channel, stride=1, use_se=use_se, se_block=se_block)\n",
    "            layers.append(resnet_block)\n",
    "        else:\n",
    "            for _ in range(1, layer_num):\n",
    "                resnet_block = block(out_channel, out_channel, stride=1, use_se=use_se)\n",
    "                layers.append(resnet_block)\n",
    "        return nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if self.use_se:\n",
    "            x = self.conv1_0(x)\n",
    "            x = self.bn1_0(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv1_1(x)\n",
    "            x = self.bn1_1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.conv1_2(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.res_base:\n",
    "            x = self.pad(x)\n",
    "        c1 = self.maxpool(x)\n",
    "\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "\n",
    "        out = self.mean(c5, (2, 3))\n",
    "        out = self.flatten(out)\n",
    "        out = self.end_point(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet18(class_num=10):\n",
    "    \"\"\"\n",
    "    Get ResNet18 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of ResNet18 neural network.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = resnet18(10)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlockBase,\n",
    "                  [2, 2, 2, 2],\n",
    "                  [64, 64, 128, 256],\n",
    "                  [64, 128, 256, 512],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num,\n",
    "                  res_base=True)\n",
    "\n",
    "\n",
    "def resnet34(class_num=10):\n",
    "    \"\"\"\n",
    "    Get ResNet34 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of ResNet34 neural network.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = resnet18(10)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlockBase,\n",
    "                  [3, 4, 6, 3],\n",
    "                  [64, 64, 128, 256],\n",
    "                  [64, 128, 256, 512],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num,\n",
    "                  res_base=True)\n",
    "\n",
    "\n",
    "def resnet50(class_num=10):\n",
    "    \"\"\"\n",
    "    Get ResNet50 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of ResNet50 neural network.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = resnet50(10)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlock,\n",
    "                  [3, 4, 6, 3],\n",
    "                  [64, 256, 512, 1024],\n",
    "                  [256, 512, 1024, 2048],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num)\n",
    "\n",
    "\n",
    "def se_resnet50(class_num=1001):\n",
    "    \"\"\"\n",
    "    Get SE-ResNet50 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of SE-ResNet50 neural network.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = se-resnet50(1001)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlock,\n",
    "                  [3, 4, 6, 3],\n",
    "                  [64, 256, 512, 1024],\n",
    "                  [256, 512, 1024, 2048],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num,\n",
    "                  use_se=True)\n",
    "\n",
    "\n",
    "def resnet101(class_num=1001):\n",
    "    \"\"\"\n",
    "    Get ResNet101 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of ResNet101 neural network.\n",
    "\n",
    "    Examples:\n",
    "        >>> net = resnet101(1001)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlock,\n",
    "                  [3, 4, 23, 3],\n",
    "                  [64, 256, 512, 1024],\n",
    "                  [256, 512, 1024, 2048],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num)\n",
    "\n",
    "\n",
    "def resnet152(class_num=1001):\n",
    "    \"\"\"\n",
    "    Get ResNet152 neural network.\n",
    "\n",
    "    Args:\n",
    "        class_num (int): Class number.\n",
    "\n",
    "    Returns:\n",
    "        Cell, cell instance of ResNet152 neural network.\n",
    "\n",
    "    Examples:\n",
    "        # >>> net = resnet152(1001)\n",
    "    \"\"\"\n",
    "    return ResNet(ResidualBlock,\n",
    "                  [3, 8, 36, 3],\n",
    "                  [64, 256, 512, 1024],\n",
    "                  [256, 512, 1024, 2048],\n",
    "                  [1, 2, 2, 2],\n",
    "                  class_num)\n",
    "\n",
    "\n",
    "\n",
    "net = resnet50(class_num=config.class_num)\n",
    "\n",
    "\n",
    "    # par_dict = torch.load(\"checkpoint/resnet50_duke2market_epoch00100.pth\", map_location='cpu')\n",
    "    # with open(\"model.txt\", 'w') as f:\n",
    "    #     for i in par_dict.keys():\n",
    "    #         f.write(i + str(par_dict[i].shape) + '\\n')\n",
    "\n",
    "    # params_list = []\n",
    "    # for name in par_dict:\n",
    "    #     param_dict = {}\n",
    "    #     parameter = par_dict[name]\n",
    "    #     param_dict['name'] = name\n",
    "    #     param_dict['data'] = Tensor(parameter.numpy())\n",
    "    #     params_list.append(param_dict)\n",
    "    # save_checkpoint(params_list, 'checkpoint/ms_resnet.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
